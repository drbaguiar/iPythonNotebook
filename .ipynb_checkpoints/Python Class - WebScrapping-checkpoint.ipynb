{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\r\n",
      "Date: Mon, 09 Nov 2015 05:06:40 GMT\r\n",
      "Server: Apache\r\n",
      "Last-Modified: Fri, 07 Aug 2015 16:39:14 GMT\r\n",
      "ETag: \"20a1817f-a7-51cbb46b621a7\"\r\n",
      "Accept-Ranges: bytes\r\n",
      "Content-Length: 167\r\n",
      "Cache-Control: max-age=604800, public\r\n",
      "Access-Control-Allow-Origin: *\r\n",
      "Access-Control-Allow-Headers: origin, x-requested-with, content-type\r\n",
      "Access-Control-Allow-Methods: GET\r\n",
      "Connection: close\r\n",
      "Content-Type: text/plain\r\n",
      "\r\n",
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fai\n",
      "r sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "#create an INET, STREAMing socket\n",
    "s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "#now connect to the web server on port 80 the normal http port\n",
    "s.connect((\"www.py4inf.com\", 80))\n",
    "s.send('GET http://www.py4inf.com/code/romeo.txt HTTP/1.0\\n\\n')\n",
    "\n",
    "while True:\n",
    "    data =s.recv(512)\n",
    "    if (len(data)<1):\n",
    "        break\n",
    "    print data\n",
    "s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>The First Page</h1>\n",
      "<p>\n",
      "If you like, you can switch to the\n",
      "<a href=\"http://www.dr-chuck.com/page2.htm\">\n",
      "Second Page</a>.\n",
      "</p>\n"
     ]
    }
   ],
   "source": [
    "# Open a web page and print line for line\n",
    "import urllib\n",
    "url = 'http://dr-chuck.com/page1.htm'\n",
    "html = urllib.urlopen(url)\n",
    "\n",
    "for line in html:\n",
    "    print line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 3, 'envious': 1, 'already': 1, 'fair': 1, 'is': 3, 'through': 1, 'pale': 1, 'yonder': 1, 'what': 1, 'sun': 2, 'Who': 1, 'But': 1, 'moon': 1, 'window': 1, 'sick': 1, 'east': 1, 'breaks': 1, 'grief': 1, 'with': 1, 'light': 1, 'It': 1, 'Arise': 1, 'kill': 1, 'the': 3, 'soft': 1, 'Juliet': 1}\n"
     ]
    }
   ],
   "source": [
    "# Open a web page and count words\n",
    "import urllib\n",
    "url = 'http://www.py4inf.com/code/romeo.txt'\n",
    "html = urllib.urlopen(url)\n",
    "\n",
    "counts = dict()\n",
    "for line in html:\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        counts[word] = counts.get(word,0)+1\n",
    "print counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What URL?:http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/comments_42.html\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup \n",
    "url = raw_input ('What URL?:')\n",
    "html = urllib.urlopen(url).read() \n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "tags = soup('a')\n",
    "\n",
    "for tag in tags:\n",
    "    print tag.get('href', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping Numbers from HTML using BeautifulSoup In this assignment you will write a Python program similar to http://www.pythonlearn.com/code/urllink2.py. The program will use urllib to read the HTML from the data files below, and parse the data, extracting numbers and compute the sum of the numbers in the file.\n",
    "\n",
    "We provide two files for this assignment. One is a sample file where we give you the sum for your testing and the other is the actual data you need to process for the assignment.\n",
    "\n",
    "Sample data: http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/comments_42.html (Sum=2553)\n",
    "Actual data: http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/comments_194064.html (Sum ends with 20)\n",
    "You do not need to save these files to your folder since your program will read the data directly from the URL. Note: Each student will have a distinct data url for the assignment - so only use your own data url for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What URL?:http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/comments_194064.html \n",
      "2720\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup \n",
    "url = raw_input ('What URL?:')\n",
    "html = urllib.urlopen(url).read() \n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('span')\n",
    "count= 0\n",
    "for tag in tags:\n",
    "   # Look at the parts of a tag\n",
    "   #print 'TAG:',tag\n",
    "   #print 'URL:',tag.get('href', None)\n",
    "   #print 'Contents:',tag.contents[0]\n",
    "   #print 'Attrs:',tag.attrs\n",
    "   count =count + int(tag.contents[0])\n",
    "\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html>\n",
       "<head>\n",
       "<title>Welcome to the comments assignment from www.pythonlearn.com</title>\n",
       "</head>\n",
       "<body>\n",
       "<h1>This file contains the actual data for your assignment - good luck!</h1>\n",
       "<table border=\"2\">\n",
       "<tr>\n",
       "<td>Name</td><td>Comments</td>\n",
       "</tr>\n",
       "<tr><td>Mya</td><td><span class=\"comments\">99</span></td></tr>\n",
       "<tr><td>Samar</td><td><span class=\"comments\">99</span></td></tr>\n",
       "<tr><td>Lani</td><td><span class=\"comments\">98</span></td></tr>\n",
       "<tr><td>Dior</td><td><span class=\"comments\">97</span></td></tr>\n",
       "<tr><td>Shayan</td><td><span class=\"comments\">94</span></td></tr>\n",
       "<tr><td>Tobias</td><td><span class=\"comments\">93</span></td></tr>\n",
       "<tr><td>Zohaib</td><td><span class=\"comments\">92</span></td></tr>\n",
       "<tr><td>Khyla</td><td><span class=\"comments\">90</span></td></tr>\n",
       "<tr><td>Erynn</td><td><span class=\"comments\">85</span></td></tr>\n",
       "<tr><td>Isobel</td><td><span class=\"comments\">85</span></td></tr>\n",
       "<tr><td>Tailee</td><td><span class=\"comments\">84</span></td></tr>\n",
       "<tr><td>Sylvanna</td><td><span class=\"comments\">81</span></td></tr>\n",
       "<tr><td>Jarvi</td><td><span class=\"comments\">79</span></td></tr>\n",
       "<tr><td>Priya</td><td><span class=\"comments\">78</span></td></tr>\n",
       "<tr><td>Porter</td><td><span class=\"comments\">77</span></td></tr>\n",
       "<tr><td>Ayeisha</td><td><span class=\"comments\">74</span></td></tr>\n",
       "<tr><td>Pieter</td><td><span class=\"comments\">72</span></td></tr>\n",
       "<tr><td>Kairn</td><td><span class=\"comments\">71</span></td></tr>\n",
       "<tr><td>Mirza</td><td><span class=\"comments\">69</span></td></tr>\n",
       "<tr><td>Zofia</td><td><span class=\"comments\">68</span></td></tr>\n",
       "<tr><td>Taiwo</td><td><span class=\"comments\">67</span></td></tr>\n",
       "<tr><td>Ngonidzashe</td><td><span class=\"comments\">65</span></td></tr>\n",
       "<tr><td>Sally</td><td><span class=\"comments\">63</span></td></tr>\n",
       "<tr><td>Kodi</td><td><span class=\"comments\">58</span></td></tr>\n",
       "<tr><td>Isabell</td><td><span class=\"comments\">54</span></td></tr>\n",
       "<tr><td>Aurea</td><td><span class=\"comments\">54</span></td></tr>\n",
       "<tr><td>Jayme</td><td><span class=\"comments\">49</span></td></tr>\n",
       "<tr><td>Clifford</td><td><span class=\"comments\">47</span></td></tr>\n",
       "<tr><td>Elisia</td><td><span class=\"comments\">45</span></td></tr>\n",
       "<tr><td>Tianqi</td><td><span class=\"comments\">44</span></td></tr>\n",
       "<tr><td>Caelen</td><td><span class=\"comments\">44</span></td></tr>\n",
       "<tr><td>Kelso</td><td><span class=\"comments\">43</span></td></tr>\n",
       "<tr><td>Zoe</td><td><span class=\"comments\">39</span></td></tr>\n",
       "<tr><td>Eren</td><td><span class=\"comments\">38</span></td></tr>\n",
       "<tr><td>Page</td><td><span class=\"comments\">38</span></td></tr>\n",
       "<tr><td>Radmiras</td><td><span class=\"comments\">37</span></td></tr>\n",
       "<tr><td>Palmer</td><td><span class=\"comments\">35</span></td></tr>\n",
       "<tr><td>Ruairidh</td><td><span class=\"comments\">34</span></td></tr>\n",
       "<tr><td>Honour</td><td><span class=\"comments\">27</span></td></tr>\n",
       "<tr><td>Maci</td><td><span class=\"comments\">25</span></td></tr>\n",
       "<tr><td>Ismail</td><td><span class=\"comments\">23</span></td></tr>\n",
       "<tr><td>Alayna</td><td><span class=\"comments\">20</span></td></tr>\n",
       "<tr><td>Kaitlan</td><td><span class=\"comments\">20</span></td></tr>\n",
       "<tr><td>Leven</td><td><span class=\"comments\">17</span></td></tr>\n",
       "<tr><td>Derick</td><td><span class=\"comments\">11</span></td></tr>\n",
       "<tr><td>Alastair</td><td><span class=\"comments\">11</span></td></tr>\n",
       "<tr><td>Aidy</td><td><span class=\"comments\">10</span></td></tr>\n",
       "<tr><td>Robyn</td><td><span class=\"comments\">9</span></td></tr>\n",
       "<tr><td>Anmol</td><td><span class=\"comments\">7</span></td></tr>\n",
       "<tr><td>Emon</td><td><span class=\"comments\">1</span></td></tr>\n",
       "</table>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup \n",
    "url = raw_input ('What URL?:')\n",
    "html = urllib.urlopen(url).read() \n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following Links in Python\n",
    "\n",
    "In this assignment you will write a Python program that expands on http://www.pythonlearn.com/code/urllinks.py. The program will use urllib to read the HTML from the data files below, extract the href= vaues from the anchor tags, scan for a tag that is in a particular position relative to the first name in the list, follow that link and repeat the process a number of times and report the last name you find.\n",
    "\n",
    "We provide two files for this assignment. One is a sample file where we give you the name for your testing and the other is the actual data you need to process for the assignment\n",
    "\n",
    "Sample problem: Start at http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Fikret.html \n",
    "Find the link at position 3 (the first name is 1). Follow that link. Repeat this process 4 times. The answer is the last name that you retrieve.\n",
    "Sequence of names: Fikret Montgomery Mhairade Butchi Anayah \n",
    "Last name in sequence: Anayah\n",
    "Actual problem: Start at: http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Maeya.html \n",
    "Find the link at position 18 (the first name is 1). Follow that link. Repeat this process 7 times. The answer is the last name that you retrieve.\n",
    "Hint: The first character of the name of the last page that you will load is: C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Cohen.html\n",
      "Cohen\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Chala.html\n",
      "Chala\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Kayleb.html\n",
      "Kayleb\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Morton.html\n",
      "Morton\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Elisa.html\n",
      "Elisa\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Joanne.html\n",
      "Joanne\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Alexis.html\n",
      "Alexis\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup \n",
    "#url = raw_input ('What URL?:')\n",
    "url = 'http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Maeya.html'\n",
    "url = 'http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Shafaq.html'\n",
    "url = 'http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Hania.html'\n",
    "url = 'http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Connal.html'\n",
    "url = 'http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Roberta.html'\n",
    "url = 'http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Ze.html'\n",
    "url = 'http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Tala.html'\n",
    "\n",
    "html = urllib.urlopen(url).read() \n",
    "soup = BeautifulSoup(html)\n",
    "#count = int(raw_input('Enter count'))\n",
    "#start = int(raw_input('Enter start'))\n",
    "start = 18\n",
    "count = 7\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "\n",
    "subtags = tags[start-1:start-1 + count]\n",
    "\n",
    "for tag in subtags:\n",
    "    print tag.get('href', None)\n",
    "    print tag.contents[0]\n",
    "    \n",
    "#html = urllib.urlopen(flurl).read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
