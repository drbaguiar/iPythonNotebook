{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What URL?: \n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<string>, line 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<string>\"\u001b[1;36m, line \u001b[1;32munknown\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "#import urllib.request  # Python 3 calls this library\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup  # This assumes a pip install\n",
    "url = input('What URL?: ')  # Python3 method is input() instead of raw_input()\n",
    "#url = raw_input ('What URL?:')\n",
    "#html = urllib.request.urlopen(url).read()  # Python3 method\n",
    "html = urllib.urlopen(url).read()  # Python3 method\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "# The parser will raise a warning if no parser is specified.\n",
    "#  Retrieve a list of the anchor tags\n",
    "#  Each tag is like a dictionary of HTML attributes\n",
    "tags = soup('a')  # Looks for tags like <a xxx> </a>\n",
    "for tag in tags:\n",
    "    print(tag.get('href', None))  # If there's no tag, the None is there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\r\n",
      "Date: Fri, 06 Nov 2015 03:56:48 GMT\r\n",
      "Server: Apache\r\n",
      "Last-Modified: Fri, 07 Aug 2015 16:39:14 GMT\r\n",
      "ETag: \"20a1817f-a7-51cbb46b621a7\"\r\n",
      "Accept-Ranges: bytes\r\n",
      "Content-Length: 167\r\n",
      "Cache-Control: max-age=604800, public\r\n",
      "Access-Control-Allow-Origin: *\r\n",
      "Access-Control-Allow-Headers: origin, x-requested-with, content-type\r\n",
      "Access-Control-Allow-Methods: GET\r\n",
      "Connection: close\r\n",
      "Content-Type: text/plain\r\n",
      "\r\n",
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fai\n",
      "r sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "#create an INET, STREAMing socket\n",
    "s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "#now connect to the web server on port 80 the normal http port\n",
    "s.connect((\"www.py4inf.com\", 80))\n",
    "s.send('GET http://www.py4inf.com/code/romeo.txt HTTP/1.0\\n\\n')\n",
    "\n",
    "while True:\n",
    "    data =s.recv(512)\n",
    "    if (len(data)<1):\n",
    "        break\n",
    "    print data\n",
    "s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>The First Page</h1>\n",
      "<p>\n",
      "If you like, you can switch to the\n",
      "<a href=\"http://www.dr-chuck.com/page2.htm\">\n",
      "Second Page</a>.\n",
      "</p>\n"
     ]
    }
   ],
   "source": [
    "# Open a web page and print line for line\n",
    "import urllib\n",
    "url = 'http://dr-chuck.com/page1.htm'\n",
    "html = urllib.urlopen(url)\n",
    "\n",
    "for line in html:\n",
    "    print line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 3, 'envious': 1, 'already': 1, 'fair': 1, 'is': 3, 'through': 1, 'pale': 1, 'yonder': 1, 'what': 1, 'sun': 2, 'Who': 1, 'But': 1, 'moon': 1, 'window': 1, 'sick': 1, 'east': 1, 'breaks': 1, 'grief': 1, 'with': 1, 'light': 1, 'It': 1, 'Arise': 1, 'kill': 1, 'the': 3, 'soft': 1, 'Juliet': 1}\n"
     ]
    }
   ],
   "source": [
    "# Open a web page and count words\n",
    "import urllib\n",
    "url = 'http://www.py4inf.com/code/romeo.txt'\n",
    "html = urllib.urlopen(url)\n",
    "\n",
    "counts = dict()\n",
    "for line in html:\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        counts[word] = counts.get(word,0)+1\n",
    "print counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What URL?:http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/comments_42.html\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup \n",
    "url = raw_input ('What URL?:')\n",
    "html = urllib.urlopen(url).read() \n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "tags = soup('a')\n",
    "\n",
    "for tag in tags:\n",
    "    print tag.get('href', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping Numbers from HTML using BeautifulSoup In this assignment you will write a Python program similar to http://www.pythonlearn.com/code/urllink2.py. The program will use urllib to read the HTML from the data files below, and parse the data, extracting numbers and compute the sum of the numbers in the file.\n",
    "\n",
    "We provide two files for this assignment. One is a sample file where we give you the sum for your testing and the other is the actual data you need to process for the assignment.\n",
    "\n",
    "Sample data: http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/comments_42.html (Sum=2553)\n",
    "Actual data: http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/comments_194064.html (Sum ends with 20)\n",
    "You do not need to save these files to your folder since your program will read the data directly from the URL. Note: Each student will have a distinct data url for the assignment - so only use your own data url for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What URL?:http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/comments_194064.html \n",
      "2720\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup \n",
    "url = raw_input ('What URL?:')\n",
    "html = urllib.urlopen(url).read() \n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('span')\n",
    "count= 0\n",
    "for tag in tags:\n",
    "   # Look at the parts of a tag\n",
    "   #print 'TAG:',tag\n",
    "   #print 'URL:',tag.get('href', None)\n",
    "   #print 'Contents:',tag.contents[0]\n",
    "   #print 'Attrs:',tag.attrs\n",
    "   count =count + int(tag.contents[0])\n",
    "\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html>\n",
       "<head>\n",
       "<title>Welcome to the comments assignment from www.pythonlearn.com</title>\n",
       "</head>\n",
       "<body>\n",
       "<h1>This file contains the actual data for your assignment - good luck!</h1>\n",
       "<table border=\"2\">\n",
       "<tr>\n",
       "<td>Name</td><td>Comments</td>\n",
       "</tr>\n",
       "<tr><td>Mya</td><td><span class=\"comments\">99</span></td></tr>\n",
       "<tr><td>Samar</td><td><span class=\"comments\">99</span></td></tr>\n",
       "<tr><td>Lani</td><td><span class=\"comments\">98</span></td></tr>\n",
       "<tr><td>Dior</td><td><span class=\"comments\">97</span></td></tr>\n",
       "<tr><td>Shayan</td><td><span class=\"comments\">94</span></td></tr>\n",
       "<tr><td>Tobias</td><td><span class=\"comments\">93</span></td></tr>\n",
       "<tr><td>Zohaib</td><td><span class=\"comments\">92</span></td></tr>\n",
       "<tr><td>Khyla</td><td><span class=\"comments\">90</span></td></tr>\n",
       "<tr><td>Erynn</td><td><span class=\"comments\">85</span></td></tr>\n",
       "<tr><td>Isobel</td><td><span class=\"comments\">85</span></td></tr>\n",
       "<tr><td>Tailee</td><td><span class=\"comments\">84</span></td></tr>\n",
       "<tr><td>Sylvanna</td><td><span class=\"comments\">81</span></td></tr>\n",
       "<tr><td>Jarvi</td><td><span class=\"comments\">79</span></td></tr>\n",
       "<tr><td>Priya</td><td><span class=\"comments\">78</span></td></tr>\n",
       "<tr><td>Porter</td><td><span class=\"comments\">77</span></td></tr>\n",
       "<tr><td>Ayeisha</td><td><span class=\"comments\">74</span></td></tr>\n",
       "<tr><td>Pieter</td><td><span class=\"comments\">72</span></td></tr>\n",
       "<tr><td>Kairn</td><td><span class=\"comments\">71</span></td></tr>\n",
       "<tr><td>Mirza</td><td><span class=\"comments\">69</span></td></tr>\n",
       "<tr><td>Zofia</td><td><span class=\"comments\">68</span></td></tr>\n",
       "<tr><td>Taiwo</td><td><span class=\"comments\">67</span></td></tr>\n",
       "<tr><td>Ngonidzashe</td><td><span class=\"comments\">65</span></td></tr>\n",
       "<tr><td>Sally</td><td><span class=\"comments\">63</span></td></tr>\n",
       "<tr><td>Kodi</td><td><span class=\"comments\">58</span></td></tr>\n",
       "<tr><td>Isabell</td><td><span class=\"comments\">54</span></td></tr>\n",
       "<tr><td>Aurea</td><td><span class=\"comments\">54</span></td></tr>\n",
       "<tr><td>Jayme</td><td><span class=\"comments\">49</span></td></tr>\n",
       "<tr><td>Clifford</td><td><span class=\"comments\">47</span></td></tr>\n",
       "<tr><td>Elisia</td><td><span class=\"comments\">45</span></td></tr>\n",
       "<tr><td>Tianqi</td><td><span class=\"comments\">44</span></td></tr>\n",
       "<tr><td>Caelen</td><td><span class=\"comments\">44</span></td></tr>\n",
       "<tr><td>Kelso</td><td><span class=\"comments\">43</span></td></tr>\n",
       "<tr><td>Zoe</td><td><span class=\"comments\">39</span></td></tr>\n",
       "<tr><td>Eren</td><td><span class=\"comments\">38</span></td></tr>\n",
       "<tr><td>Page</td><td><span class=\"comments\">38</span></td></tr>\n",
       "<tr><td>Radmiras</td><td><span class=\"comments\">37</span></td></tr>\n",
       "<tr><td>Palmer</td><td><span class=\"comments\">35</span></td></tr>\n",
       "<tr><td>Ruairidh</td><td><span class=\"comments\">34</span></td></tr>\n",
       "<tr><td>Honour</td><td><span class=\"comments\">27</span></td></tr>\n",
       "<tr><td>Maci</td><td><span class=\"comments\">25</span></td></tr>\n",
       "<tr><td>Ismail</td><td><span class=\"comments\">23</span></td></tr>\n",
       "<tr><td>Alayna</td><td><span class=\"comments\">20</span></td></tr>\n",
       "<tr><td>Kaitlan</td><td><span class=\"comments\">20</span></td></tr>\n",
       "<tr><td>Leven</td><td><span class=\"comments\">17</span></td></tr>\n",
       "<tr><td>Derick</td><td><span class=\"comments\">11</span></td></tr>\n",
       "<tr><td>Alastair</td><td><span class=\"comments\">11</span></td></tr>\n",
       "<tr><td>Aidy</td><td><span class=\"comments\">10</span></td></tr>\n",
       "<tr><td>Robyn</td><td><span class=\"comments\">9</span></td></tr>\n",
       "<tr><td>Anmol</td><td><span class=\"comments\">7</span></td></tr>\n",
       "<tr><td>Emon</td><td><span class=\"comments\">1</span></td></tr>\n",
       "</table>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup \n",
    "url = raw_input ('What URL?:')\n",
    "html = urllib.urlopen(url).read() \n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following Links in Python\n",
    "\n",
    "In this assignment you will write a Python program that expands on http://www.pythonlearn.com/code/urllinks.py. The program will use urllib to read the HTML from the data files below, extract the href= vaues from the anchor tags, scan for a tag that is in a particular position relative to the first name in the list, follow that link and repeat the process a number of times and report the last name you find.\n",
    "\n",
    "We provide two files for this assignment. One is a sample file where we give you the name for your testing and the other is the actual data you need to process for the assignment\n",
    "\n",
    "Sample problem: Start at http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Fikret.html \n",
    "Find the link at position 3 (the first name is 1). Follow that link. Repeat this process 4 times. The answer is the last name that you retrieve.\n",
    "Sequence of names: Fikret Montgomery Mhairade Butchi Anayah \n",
    "Last name in sequence: Anayah\n",
    "Actual problem: Start at: http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Maeya.html \n",
    "Find the link at position 18 (the first name is 1). Follow that link. Repeat this process 7 times. The answer is the last name that you retrieve.\n",
    "Hint: The first character of the name of the last page that you will load is: C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter count4\n",
      "Enter start3\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Aniqa.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Ogheneruno.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Montgomery.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Owain.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Haniyah.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Anona.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Edyn.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Dallace.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Zoe.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Kiarash.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Tracy.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Carmyle.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Zahraa.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Alanys.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Airidas.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Melisa.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Vivian.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Margaret.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Hajra.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Ajooni.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Alexanne.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Sudais.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Seb.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Christin.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Jaimie.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Jennah.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Landon.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Mea.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Cacie.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Colton.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Mitchel.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Chintu.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Hyden.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Chrystal.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Lincon.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Jaden.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Roma.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Manolo.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Clio.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Teos.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Rihonn.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Griffin.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Conley.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Xiao.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Dhyia.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Manahil.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Diona.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Dharam.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Danielle.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Rori.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Lang.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Sabila.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Zoha.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Jemma.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Silvana.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Asal.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Dillon.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_CJ.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Joanna.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Atal.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Callun.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Anubhav.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Coray.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Graeme.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Chrissie.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Ayub.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Heather.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Katie.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Inaara.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Siddhant.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Salymat.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Shahd.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Anaya.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Kevaugh.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Thumbiko.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Xida.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Alaska.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Shonagh.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Kaiya.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Khadija.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Kieron.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Filip.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Dorothy.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Kallan.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Mena.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Abbie.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Amyleigh.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Annalise.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Carrich.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Rachel.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Etinosa.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Amie.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Lisa.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Liv.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Baylie.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Jubin.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Kacie.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Falyn.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Conli.html\n",
      "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Cohen.html\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup \n",
    "#url = raw_input ('What URL?:')\n",
    "#url = 'http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Fikret.html'\n",
    "html = urllib.urlopen(url).read() \n",
    "soup = BeautifulSoup(html)\n",
    "count = int(raw_input('Enter count'))\n",
    "start = int(raw_input('Enter start'))\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    " print tag.get('href', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Fikret.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Montgomery.html\">Montgomery</a>,\n",
       " <a href=\"http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Owain.html\">Owain</a>,\n",
       " <a href=\"http://pr4e.dr-chuck.com/tsugi/mod/python-data/data/known_by_Haniyah.html\">Haniyah</a>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags[2:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
